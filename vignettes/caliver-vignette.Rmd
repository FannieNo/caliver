---
title: "caliver: CALIbration and VERification of gridded model outputs"
author: ""
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo=FALSE}
knitr::opts_chunk$set(
  comment = '#>',
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  eval = FALSE
)
```

The package [caliver](https://cran.r-project.org/package=caliver) contains utility functions for the post-processing, calibration and validation of gridded model outputs. Initial test cases include the outputs of the following forest fire models: GEFF and RISICO.

## Dependencies and installation

Install [cdo](https://code.zmaw.de/projects/cdo/wiki), [gunzip](http://www.gzip.org/), [gdal](http://www.gdal.org/), [netcdf4](http://www.unidata.ucar.edu/software/netcdf/) and the following packages before attempting to install caliver:

```{r}
packs <- c("devtools", "graphics", "grDevices", "knitr", "latticeExtra", 
           "raster", "rasterVis", "rgdal", "sp", 
           "stats", "testthat", "rmarkdown", "rworldmap")
new.packages <- packs[!(packs %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```

Get the development version from github using [devtools](https://github.com/hadley/devtools):

```{r}
devtools::install_github("ecmwf/caliver")
```

Load the caliver package:

```{r}
library("caliver")
```

## Set working directory
Define the data folder, where the results will be stored
```{r}
setwd("/var/tmp/moc0/geff/results")
```

## Workflow type A: decompress single variable files and merge them over the time dimension
GEFF output files may be stored in a bunch of compressed archives, typically in \*.gz format. If each file contains only one variable, this workflow can be used to decompress and merge all the files over the time dimension.
The first step is to define the data folder containing the compressed archives and decompress them all. The operation is preformed by the __decompressGZ()__ function and will, by default, remove the compressed archive once the information has been extracted (from \*.nc.gz to \*.nc). If you wish to keep the *.gz files set the argument `keep = TRUE`.

```{r}
decompressGZ(dirs = "/var/tmp/moc0/forestfire", keep = FALSE)
```

Then, you can use the __mergetime()__ function to merge all the extracted files over the time dimension. By default the function mergetime will try to merge all the \*.nc files in the `dir` folder. If you want to restrict the operation to few files, you can use the argument `startingString`.

```{r}
# Merge all the files (with name starting with *startingString*) over the time dimension
mergedFile <- mergetime(dirs = "/var/tmp/moc0/forestfire", 
                        startingString = "geff_reanalysis_an_fwis_fwi_")
```

## Workflow type B: Calculate quantiles
Given a netcdf file with 3 dimensions (lat, lon and time), we can calculate quantiles cell by cell:
```{r}
mergedFile <- paste0(getwd(), "/outfile.nc")
```

Get maps for given quantiles
```{r}
probsMaps <- getGriddedCDF(ncfile = mergedFile, 
                           probs = c(50, 75, 90, 99))
```

The above command generates 4 probability maps that can be further manipulated and/or plotted.

## Workflow type C: Plot multiple maps
In order to conveniently plot the results of workflow B, the maps should be shifted horizontally. This is because data from global climate models are set to have longitude between 0 and 360, while for the majority of other standard longitudes vary between -180 and +180 degrees. The shift (or rotation) can be achieved in two ways: either using the \code{raster::rotate()} function or \code{sellonlatbox} from the cdo library.

```{r}
# Option A: use the raster::rotate() function
shiftedMaps <- lapply(X = probsMaps, FUN = function(x){raster::rotate(x)})

# Option B: use sellonlatbox from the cdo library
listOfFiles <- as.character(lapply(X = probsMaps, 
                                   FUN = function(x){x@file@name}))
shiftedMaps <- raster::stack(shiftMap(inFile = listOfFiles))
```

Plot maps with background
```{r}
p <- rasterVis::levelplot(raster::stack(shiftedMaps), 
                          col.regions = rev(grDevices::heat.colors(20)), 
                          colorkey = list(space = "right"))
# Define a background map
backgroundMap <- rworldmap::getMap(resolution = "low")
p <- p + 
  latticeExtra::layer(sp::sp.lines(backgroundMap, lwd=0.8, col="darkgray"))

print(p)
```

The same plot can be achieved using the command \code{plotPercentiles()}. This has the advantage of incorporating rotation of longitudes, if necessary.
```{r}
plotPercentiles(maps = probsMaps, backgroundMap = TRUE, rotateMap = TRUE)
plotPercentiles(maps = shiftedMaps, backgroundMap = TRUE, rotateMap = FALSE)
```

## Workflow type D: extract one variable from multi-variable files, calculate CDF and plot quantiles
GEFF output files may store multiple variables. This workflow can be used to extract one variable from multiple files and merge the information over the time dimension. Once the merged file is available it is possible to repeat the operations illustrated in workflows B and C to calculate quantiles and plot maps, respectively.

The first step is to define the data folder where all the GEFF multi-variable files are stored (not in compressed format) and the name of the output file where to stored the merged data.
```{r}
dirs <- "/var/tmp/moc0/geff/data/"
varname <- "fwi"
ncfile <- paste0(varname,".nc")
```

Merge all the files (with name starting with *startingString*) over the time dimension
```{r}
mergedFile <- mergetime(dirs = dirs,
                        varname = varname, 
                        startingString = "",
                        recursive = TRUE, 
                        outFile = ncfile)
```

Calculate CDF and percentiles
```{r}
listOfMaps <- getGriddedCDF(mergedFile)
```

Plot maps
```{r}
plotPercentiles(maps = listOfMaps, rotateMap = TRUE)
```

## Workflow type E: mask unrealistic values
In absence of vegetation the risk of ignition reduces considerably, regardless of the state of the soil. To exclude these areas (deserts, glaciers, urban areas, etc.) we apply a mask to the raster map of percentiles calculated above. We identify non-vegetated areas using the fuel_model map provided by JRC. 
```{r}
mergedFile <- "fwi.nc"

maskedMaps <- getGriddedCDF(ncfile = mergedFile, 
                            probs = c(50, 75, 90, 99), 
                            mask = "fuel_model")
# Plot
plotPercentiles(maps = maskedMaps, rotateMap = TRUE)
```

The layer "fuel\_model" is sored in the data folder of the caliver package and can be used to mask any other raster, e.g. the maps of fwi-dc-ffmc percentiles.

## Workflow type F: visualise the 99th percentile for Europe only (using GFED basis regions)
The example below shows how to plot pre-defined fwi percentiles for Europe using an irregular polygon provided by the GFED basis regions (available here: www.globalfiredata.org/data.html)
```{r}
mergedFile <- "fwi.nc"

# Mask & crop
croppedMaps <- getGriddedCDF(ncfile = mergedFile, 
                             probs = 99,
                             mask = "fuel_model",
                             region = "EURO")

# Plot
plotPercentiles(maps = croppedMaps, backgroundMap = TRUE, rotateMap = FALSE)
plotPercentiles(maps = croppedMaps, backgroundMap = FALSE, 
                rotateMap = TRUE, cropMap = TRUE, region = "EURO")
```

Note that to centre the map to Europe we need to crop the map twice!

## Workflow type G: calculate CDF by region
This workflow shows a generalisation of workflow F, in which we can calculate the CDF (using various quantiles) by region. The first step is to get a map for a reasonable number of percentiles so that the CDF can be constructed smoothly
```{r}
library(ggplot2)

# Define the input data folder (this contains all GEFF-reanalysis data)
dirs <- "/var/tmp/moc0/geff/data/"

# Define the list of regions
regions <- c("GLOB", "BONA", "TENA", "CEAM", "NHSA", "SHSA", "EURO", 
             "MIDE", "NHAF", "SHAF", "BOAS", "CEAS", "SEAS", "EQAS", "AUST")

data(GFEDregions)
data(fuelmodel)
  
for (varname in c("fwi", "dc", "ffmc")){
  
  ncfile <- paste0(varname,".nc")
  
  # Merge all the files (with name starting with *startingString*) 
  # over the time dimension
  mergedFile <- mergetime(dirs = dirs,
                          varname = varname,
                          startingString = "",
                          recursive = TRUE,
                          outFile = ncfile)
  
  # Set the list of probabilities to calculate
  probs <- c(seq(5,95, by = 5),99)
  ncfile <- paste0(getwd(), "/", varname, ".nc")
  # Calculate percentiles (globally)
  listOfMaps <- getGriddedCDF(ncfile = ncfile, probs = probs)
  
  # Then create CDF curves by averaging the percentiles over a given area
  CDF <- data.frame(matrix(NA, 
                           ncol = length(regions) + 1, 
                           nrow = length(probs)))
  names(CDF) <- c("percentile", regions)
  CDF$percentile <- probs
  
  for (region in regions){
    
    print(region)
    j <- which(names(CDF) == region) # counter over the columns
    
    # Mask the percentile maps using regional masks
    for (prob in probs){
      
      i <- which(CDF$percentile == prob)
      globalMap <- raster::raster(paste0(varname, "_", prob, ".nc"))
      globalMap_fuelonly <- raster::mask(globalMap, fuelmodel)
      
      if (region == "GLOB"){  
        maskedMap <- globalMap_fuelonly
      }else{
        regionMap <- regionalMask(region)
        maskedMap <- raster::mask(globalMap_fuelonly, regionMap)
      }
      
      CDF[i, j] <- round(mean(maskedMap@data@values, na.rm=TRUE), 3)
      
    }
    
  }
  saveRDS(CDF, paste0(varname, "_CDF.rds"))

  CDFmelt <- reshape2::melt(data = CDF, id.vars = names(CDF)[1])
  ggplot(data = CDFmelt, aes(x=value, y=percentile)) +
    geom_line(aes(colour=variable)) + 
    ggtitle("CDF by GFED basis regions")
}
```

## Workflow type H: calculate thresholds by region
Some pre-defined quantiles can be used to define risk thresholds. In this workflow we show the case in which low-medium-high-extreme risk correspond to the 50th-75th-90th-99th percentile, respectively.
```{r}
# Set working directory
setwd("/var/tmp/moc0/geff/CDF")
# Define the list of regions
regions <- c("GLOB", "BONA", "TENA", "CEAM", "NHSA", "SHSA", "EURO", 
             "MIDE", "NHAF", "SHAF", "BOAS", "CEAS", "SEAS", "EQAS", "AUST")
thresholds <- data.frame(matrix(NA, ncol = 2 + length(regions), nrow = 3*4))
names(thresholds) <- c("Varname", "Potential Risk Threshold", regions)
thresholds$`Potential Risk Threshold` <- c("Low", "Medium", "High", "Extreme")
i <- 1
for (varname in c("fwi", "dc", "ffmc")){
  df <- readRDS(paste0(varname, "_CDF.rds"))
  thresholds[i:(i+3),1] <- varname
  thresholds[i,3:dim(thresholds)[2]] <- df[which(df$percentile == 50), 2:dim(df)[2]]
  thresholds[i+1,3:dim(thresholds)[2]] <- df[which(df$percentile == 75), 2:dim(df)[2]]
  thresholds[i+2,3:dim(thresholds)[2]] <- df[which(df$percentile == 90), 2:dim(df)[2]]
  thresholds[i+3,3:dim(thresholds)[2]] <- df[which(df$percentile == 99), 2:dim(df)[2]]
  i <- i + 4
}

thresholds[,3:17] <- thresholds[,3:17]
saveRDS(thresholds, "thresholds.RDS")
```

## Workflow type I: validate burned areas
If there are observations of actual fire events, these can be used to validate the thresholds defined in workflow H. The methodology we use here is as follows:
  
  1. We merge all the fire events (by day - global extension) in one 3 dimensional (lat-lon-time) file (raster brick) in which the only variable measured is called "BurnedArea"
  2. If the raster brick containing observations has different resolution (e.g. 4 times higher) and extent compared to the FWI threshold, we aggregate (using the mean of the percentage coverage) and resample the observations.
  3. Create a single layer raster in which each cell can assume only binary values: 0 if thre was no fire event over time, 1 if there has been at least one event (with burned area > 20% of the cell extent) over time.
  4. Summarise data in a data.frame where the first column is the FWI and the second column is the burned area outcome (0/1)
  5. Generate density plots groupped by FWI threshold.
  
```{r}
# Set working directory
setwd("/var/tmp/moc0/geff/")

obs <- mergetime(dirs = "./GFED4_BurnedAreas/NetCDFdaily2000-2014",
                 varname = "BurnedArea", startingString = "",
                 recursive = FALSE, outFile = "BurnedArea.nc")
obs <- raster::brick(obs)
# obs <- raster::brick("/var/tmp/moc0/geff/BurnedArea.nc")

# Aggregate every 4 cells to match resolution of FWI layer
obs_aggregated <- raster::aggregate(obs, fact = 4, fun=mean, progress = 'text')

for (thresholdFWI in c(50, 75, 90, 99)){
  
  i <- which(c(50, 75, 90, 99) == thresholdFWI)
  
  fwi <- raster::raster(paste0("/var/tmp/moc0/geff/FWI_", thresholdFWI, ".nc"))
  
  if (thresholdFWI == 50) {
    obs_aggregated_resampled <- raster::resample(obs_aggregated, fwi)
    # Check rasters have same extent, neede for comparisons
    # raster::compareRaster(obs_aggregated_resampled, fwi)
    saveRDS(obs_aggregated_resampled, "obs_aggregated_resampled.rds")
    # obs_aggregated_resampled <- readRDS("obs_aggregated_resampled.rds")
    
    thresholdBurntArea <- 20
    burnt <- raster::calc(obs_aggregated_resampled,
                          function(x){ifelse(test = x < thresholdBurntArea/100,
                                             yes = 0, no = 1)})
    burnt_sum <- raster::stackApply(x = burnt, indices = 1, fun = sum)
    
    burnt <- data.frame(as.vector(matrix(burnt_sum)))
    idx <- which(burnt > 0)
    names(burnt) <- "BurnedCount"
    burnt$BurnedAreas <- FALSE
    burnt$BurnedAreas[idx] <- TRUE
  }
  
  burnt <- cbind(burnt, as.vector(matrix(fwi)))
  names(burnt)[2+i] <- paste0("FWI", thresholdFWI)
}

library(ggplot2)
library(reshape2)
burntMelt <- melt(burnt, id.vars = c("BurnedCount", "BurnedAreas"))
saveRDS(burntMelt, "burntMelt.rds")
# burntMelt <- readRDS("burntMelt.rds")
ggplot(burntMelt, aes(x=value, fill=BurnedAreas)) + 
  geom_density(alpha=.3, color = NA) + 
  facet_wrap(~ variable, scales = "free_y") + theme_bw() + xlab("")
```

## Workflow type J: get example risk maps for Italy
This example shows how to read the RISICO Binary files for two timesteps and aggregate the values in space and time using the `getPercRiskIndex()' function over the Italian Provinces (available as shapefile). Finally, it visualises the output and the input on a map.

```{r}
library(mapdata)
library(maptools)

setwd("/home/mo/moc0/Mirko/")

source("readRisicoBinary.R")
source("getPercRiskIndex.R")

filename1 <- "data/RISICO2015_VPPF_201611140300"
filename2 <- "data/RISICO2015_VPPF_201611140600"
r_values_1 <- readRisicoBinary(filename1)
r_values_2 <- readRisicoBinary(filename2)

r_stack <- stack(c(r_values_1, r_values_2))

p_italy <- readShapePoly("data/province_ISTAT2001.shp")

map("worldHires","Italy", col="gray90", fill=TRUE)
plot(r_values_1, add=T)
plot(p_italy, add=T)

plot(r_values_2, add=F)
plot(p_italy, add=T)

# Extract raster values to list object
r.index = getPercRiskIndex(r_stack, p_italy, perc.val=50, mod="lt")
p_italy@data$VALUE <- r.index

vec_val <- unlist(p_italy@data$VALUE)
max_val <- max(vec_val)
min_val <- min(vec_val)
plot(p_italy, col=gray((vec_val-min_val)/(max_val-min_val)))
```
